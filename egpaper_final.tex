\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Articulated Human Pose Estimation with Deep Convolutional Neural Networks}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
\end{abstract}

%%%%%%%%% BODY TEXT
%-------------------------------------------------------------------------
\section{Introduction}
\begin{itemize}
  \item Pose Estimation popular and challenging
  \item Flexible Mixtures of Parts Model and HOG feature
  \item Deep Convolutional Neural Networks
  \item Deformable Part Models is Neural Network
\end{itemize}

%-------------------------------------------------------------------------
\section{Reference Analysis}
{\bf Deformable Part Models is Neural Network} DPM uses max-pooling to represent the pairwise geometric relationship. After computing the heat maps for a root and a part, the part add the geometric kernel to its heat map and then do max-pooling and shifting to become the final message passer to the root heat map. If want to learn a kernel, that kernel will be a plus kernel which is not consistent with convolutional neural networks. The interesting property of this method is that joint structured prediction can be made if the graphical model is a tree structure, max-pooling can record the best part location.

{\bf LeCun's Paper} uses weighted-average pooling or convolution to represent the pairwise geometric relationship. After computing the heat maps for a root and a part, the part multiple a geometric filter to its heat map and then do the shifting to become the final message passer to the root heat map. If want to learn a kernel, LeCun's way is better. And also, LeCun's way can be done for multiple parts simultaneously like convolution, although it's not like DPM but every part can output its most probably location by looking at the final heat maps. 

{\bf Comparison} DPM's way can benefit from the pure geometric prior, which is that given no image evidence what is the most probable geometric relationship between parts. While LeCun's way is easier to learn the geometric filters in a convolutional neural net framework.

LeCun's way can also perform a max-pooling after the geometric filtering. There are multiple meaning for doing this or not doing this. If do, the max-pooling kernel size may be the same as the geometric filter size, it represents the inference based on a part to a root, max-pooling means location invariance, which could be useful.

{\bf Anchor position} No matter which method, anchor position should still be very important. Given one part with a particular type, the other part location should be restricted around an anchor position to make inference meaningful.

%-------------------------------------------------------------------------
\section{Related Work}
\begin{itemize}
  \item Flexible Mixtures of Parts Model and HOG feature
  \item Deep Convolutional Neural Networks
  \item Deformable Part Models is Neural Network
\end{itemize}

%-------------------------------------------------------------------------
\section{Model} 

\subsection{Convolutional Part Detector}
The first part of the model is a discriminative part detector with deep learning features. It should work very well given a reasonable receptive field. The size of the part filters is not very large. If the part size is larger, it probably will need to model more mixture information inside each part, making it hard to generalize.

\subsection{Higher Level Spatial Model}
Given these small part detection results, the heat maps, geometry relationships will be there. A max pooling is a speical form of the distance transform pooling. The main difference will be the anchor position. Given an anchor position, the final map is first pooled, then shift by an anchor position to become the message passing to the next part. Either guassian function or a uniform function can be assumed, or any possible function can be assumed, the distance transform pooling hence can try to learn a special convolutional kernel.  

\subsection{Unified Model}

\section{Inference}

\section{Learning}

\subsection{Learning Unary Terms}

Learning unary part template requires positive and negative training examples. Positive examples, if allow multiple possible positions may lose its spatial invariance. Hence latent updating could be a step for future usage. 

Determining part size: this should be roughly determined with data, actually there is no exact solution, maybe 24x24 or 48x48 is a good choice. If 24x24, then 3x3 convolution kernel size, with 3x3 max pooling size, there will be 3 convolution and 2 max pooling 

\subsection{Learning Pairwise Terms}

\subsection{Joint learning}

\subsection{Latent Update}

%-------------------------------------------------------------------------
\section{Experiments}

\subsection{Dataset}

\subsection{Implementation Details}
\begin{itemize}
  \item Only set feedback on RELU layer 
  \item Train multi-class feedback model with hidden variable sharing
\end{itemize}

\subsection{Visualization of feedbacks}
A figure shows Feedback vs No-Feedback visualization on fc8

Key story: Feedback surpress noise and extract salient part region and contexts

\subsection{Where to Add Feedback}
A figure shows Feedback vs No-Feedback visualization on conv5 etc. \\

Key story: 1. Not all layers need feedback 2. Feedback surpress noisefor high level layers

\subsection{Are the Activation Similar for different classes}
A figure shows Feedback visualization on fc8 for different classes \\

Key story: 1. Feedbacks are different for different classes in the same image. 2. Some similar classes share similar feedbacks

\subsection{Are feedback helpful for recognition?}

\subsection{Are feedback useful for localization}
A figure shows Feedback visualization for different object classes for the same image (a little similar to above)

\subsection{Are feedback robust to noise}
A figure shows Feedback visualization for an image producing a different class label than ground truth (similar as the \"intriguing property\" paper)

\subsection{How iterative update changes visualization}
A figure shows the visualization with update iteration

\subsection{Are feedback helpful for multi-task learning?}

%-------------------------------------------------------------------------

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

Finally, you may feel you need to tell the reader that more details can be
found elsewhere, and refer them to a technical report.  For conference
submissions, the paper must stand on its own, and not {\em require} the
reviewer to go to a techreport for further details.  Thus, you may say in
the body of the paper ``further details may be found
in~\cite{Authors14b}''.  Then submit the techreport as additional material.
Again, you may not assume the reviewers will read this material.

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Method & Frobnability \\
\hline\hline
Theirs & Frumpy \\
Yours & Frobbly \\
Ours & Makes one's heart Frob\\
\hline
\end{tabular}
\end{center}
\caption{Results.   Ours is better.}
\end{table}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
